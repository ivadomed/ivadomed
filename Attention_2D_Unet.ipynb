{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Module\n",
    "from torch.nn import init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Att_Module(Module):\n",
    "    \"\"\"Attention module is implemented inspired by the following article:\n",
    "       https://arxiv.org/abs/1807.06521. \n",
    "       CBAM attention module incorporates both dorsal (where) and ventral (what) stram of attention mechanism.\n",
    "       base_n_filter (int): Number of base filters in the U-Net.\n",
    "       r_ratio (int): reduction ratio that influences the hidden layer neuron number. it reduces the number of neurons \n",
    "                      by a factor of r_ration, the unreduced number of Neurons is equal to the number of channels in \n",
    "                      the batch\n",
    "       att_kernel_size (int):  indicated the kernel size fo the convolution filtering in spatial attention (size in paper=7)\n",
    "       \"\"\"\n",
    "    \n",
    "    def __init__(self,base_n_filter = 64, r_ratio=16, att_kernel_size=7):\n",
    "        \n",
    "        super(Att_Module, self).__init__()\n",
    "        \n",
    "        self.in_Feat = base_n_filter # input feature tensor\n",
    "        self.R_ratio = r_ratio # reduction ratio for FCN\n",
    "        self.Att_K = att_kernel_size    \n",
    "        self.GAP = nn.AdaptiveAvgPool2d((1,1)) # for global average pooling\n",
    "        self.GMP =  nn.AdaptiveMaxPool2d((1,1)) # for global max pooling\n",
    "        self.FCN_1 = nn.Linear(self.in_Feat,self.in_Feat//self.R_ratio)\n",
    "        self.FCN_2 = nn.Linear(self.in_Feat//self.R_ratio,self.in_Feat)        \n",
    "        self.CNN = nn.Conv2d(2,1,kernel_size=self.Att_K,padding=(self.Att_K-1)//2)        \n",
    "        \n",
    "        \n",
    "    def forward(self, feat_tensor):\n",
    "        \n",
    "        ################## CHANNLE ATTENTION #################\n",
    "        x = self.GAP(feat_tensor)\n",
    "        x = self.FCN_1(x.view(-1,x.size()[1])) # shape match for FCN\n",
    "        x = self.FCN_2(x)\n",
    "        \n",
    "        y = self.GMP(feat_tensor)\n",
    "        y = self.FCN_1(y.view(-1,y.size()[1])) # shape match for FCN\n",
    "        y = self.FCN_2(y)\n",
    "        \n",
    "        ch_out = F.sigmoid(x+y)\n",
    "        ch_out = ch_out.unsqueeze(dim=2).unsqueeze(dim=3) # expected dimension fixed\n",
    "        ch_out = ch_out.repeat(1,1,feat_tensor.size()[2],feat_tensor.size()[3]) # repeat a vector to expected batch size   \n",
    "              \n",
    "        feat_tensor_ch = ch_out*feat_tensor # elemenwise multiplication of input to channel att\n",
    "        \n",
    "        ####################### Spatial attention ###############\n",
    "        max_spat, indices = torch.max(feat_tensor_ch.view(-1,feat_tensor_ch.size()[1]), dim=1) # INPUT IS \n",
    "        max_spat = max_spat.view(-1,feat_tensor_ch.size()[2],feat_tensor_ch.size()[3]) # max_pooling on channels\n",
    "        \n",
    "        avg_spat = torch.mean(feat_tensor_ch.view(-1,feat_tensor_ch.size()[1]),\n",
    "                            dim=1).view(-1,feat_tensor_ch.size()[2],feat_tensor_ch.size()[3])# avg_pool on channles\n",
    "        \n",
    "        avg_spat = avg_spat.unsqueeze(dim=1) \n",
    "        max_spat = max_spat.unsqueeze(dim=1)\n",
    "        \n",
    "        z = torch.cat([max_spat,avg_spat], dim=1)\n",
    "        \n",
    "        spat_att = F.sigmoid(self.CNN(z))\n",
    "        spat_att = spat_att.repeat(1,feat_tensor.size()[1],1,1) # repeat a tensor to expected batch size (num samples) \n",
    "        feat_tensor_ch_spat = spat_att*feat_tensor # elemenwise multiplication of input to channel-spatial att\n",
    "        \n",
    "        ######################## FINAL ATTENTION MODULE ############\n",
    "        Final_att = feat_tensor_ch_spat + feat_tensor    \n",
    "        \n",
    "        print(f\"The input batch size is {feat_tensor.size()}\")  \n",
    "\n",
    "        print(f\"The output attention batch size is {Final_att.size()}\")  \n",
    "        \n",
    "        return Final_att\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv(in_c, out_c, kernel_size):\n",
    "    ## DEFINING DOUBLE CONVOLUTION\n",
    "    conv = nn.Sequential(nn.Conv2d(in_c,out_c,kernel_size, padding=(kernel_size-1)//2),\n",
    "              nn.ReLU(inplace=True),\n",
    "              nn.Conv2d(out_c,out_c,kernel_size, padding=(kernel_size-1)//2),\n",
    "              nn.ReLU(inplace=True))\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENCODER(Module):\n",
    "    def __init__(self,in_channel=1, depth=3, n_filters=64, kernel_size=3, attention=True, r_ratio=16, att_kernel_size=7):\n",
    "        \n",
    "        super(ENCODER, self).__init__()\n",
    "        \n",
    "        self.Depth =depth\n",
    "        self.base_n_filter =n_filters\n",
    "        self.R_ratio = r_ratio\n",
    "        self.Att_K = att_kernel_size \n",
    "        maxpool2D = nn.MaxPool2d(2,2)                                     \n",
    "        self.Down_path = nn.ModuleList() # CREATING A LIST FOR ENCODER LAYERS\n",
    "        self.Down_path.append(double_conv(in_channel,n_filters,kernel_size)) # FIRST CONV2d LAYER\n",
    "        self.Down_path.append(maxpool2D) #FIRST MAXPOOL LAYER\n",
    "        self.attention = attention\n",
    "        \n",
    "        for i in range(depth-1):\n",
    "            \n",
    "            self.Down_path.append(double_conv(n_filters*(2**i),n_filters*(2**(i+1)),kernel_size))\n",
    "            self.Down_path.append(maxpool2D)\n",
    "            \n",
    "        self.conv_bottom = double_conv(n_filters*(2**(self.Depth-1)),n_filters*(2**(self.Depth)),kernel_size)\n",
    "        \n",
    "    def forward(self, image):\n",
    "                \n",
    "        feature = []\n",
    "        #FIRST BLOCK                                                            \n",
    "        x = self.Down_path[0](image)\n",
    "        print(x.size())\n",
    "        \n",
    "        if self.attention:\n",
    "            self.attention = Att_Module(base_n_filter =self.base_n_filter, r_ratio=self.R_ratio, att_kernel_size=self.Att_K)\n",
    "            x = self.attention(x)+x\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "        feature.append(x)\n",
    "        x = self.Down_path[1](x)\n",
    "        \n",
    "        # Down-sampling path\n",
    "        for j in range(1,self.Depth):\n",
    "            x = self.Down_path[2*j](x)\n",
    "            if self.attention:\n",
    "                \n",
    "                self.attention = Att_Module(base_n_filter =self.base_n_filter*(2**j),r_ratio=self.R_ratio*(2**j),\n",
    "                                            att_kernel_size=self.Att_K)\n",
    "                x = self.attention(x)+x\n",
    "              \n",
    "            else:\n",
    "                pass\n",
    "            feature.append(x)\n",
    "            print(x.size())\n",
    "            x = self.Down_path[2*j+1](x)\n",
    "            \n",
    "        # Bottom level\n",
    "        x = self.conv_bottom(x)\n",
    "        feature.append(x) # save the bottom layer in feature list\n",
    "        print(x.size())\n",
    "        return feature\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DECODER(Module):\n",
    "    def __init__(self,out_channel=1, depth=3, n_filters=64, kernel_size=3):\n",
    "        super(DECODER,self).__init__()\n",
    "        \n",
    "        self.Up_path = nn.ModuleList() # CREATING A LIST FOR DECODER LAYERS\n",
    "        self.Depth = depth\n",
    "        self.Out = out_channel\n",
    "        \n",
    "        for h in range(0,self.Depth):\n",
    "            \n",
    "            self.Up_path.append(nn.ConvTranspose2d((n_filters*(2**self.Depth))//(2**h), \n",
    "                                                   (n_filters*(2**(self.Depth-1)))//(2**h),kernel_size,stride=2,\n",
    "                                                   padding=(kernel_size-1)//2))\n",
    "                                \n",
    "            self.Up_path.append(double_conv(n_filters*(2**self.Depth)//(2**h),\n",
    "                                            n_filters*(2**(self.Depth-1))//(2**h),kernel_size))\n",
    "            \n",
    "        self.conv_top = double_conv(n_filters,self.Out,kernel_size)\n",
    "        \n",
    "    def forward(self, feature):\n",
    "        \n",
    "        x = feature[-1]\n",
    "        \n",
    "        for i in range(0,self.Depth):\n",
    "                        \n",
    "            x = self.Up_path[2*i](x)\n",
    "          \n",
    "            feature_T = torch.tensor(feature[(self.Depth-1)-i])\n",
    "            x = F.interpolate(x,size=(feature_T.size()[2],feature_T.size()[3]),mode='bilinear')\n",
    "            x = torch.cat([x,feature[(self.Depth-1)-i]], dim =1)\n",
    "            x = self.Up_path[2*i+1](x)\n",
    "        \n",
    "        out = F.relu(self.conv_top(x)) # different kinds of activation function can be used\n",
    "        print(f\"the ouput size is {out.size()}\")\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 322, 322])\n",
      "The input batch size is torch.Size([2, 64, 322, 322])\n",
      "The output attention batch size is torch.Size([2, 64, 322, 322])\n",
      "The input batch size is torch.Size([2, 128, 161, 161])\n",
      "The output attention batch size is torch.Size([2, 128, 161, 161])\n",
      "torch.Size([2, 128, 161, 161])\n",
      "The input batch size is torch.Size([2, 256, 80, 80])\n",
      "The output attention batch size is torch.Size([2, 256, 80, 80])\n",
      "torch.Size([2, 256, 80, 80])\n",
      "torch.Size([2, 512, 40, 40])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashkan\\.conda\\envs\\pytorchenv\\lib\\site-packages\\ipykernel_launcher.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the ouput size is torch.Size([2, 2, 322, 322])\n",
      "torch.Size([2, 2, 322, 322])\n",
      "tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.2304, 0.2376, 0.2439,  ..., 0.2433, 0.2395, 0.2341],\n",
      "          [0.2303, 0.2441, 0.2528,  ..., 0.2479, 0.2440, 0.2394],\n",
      "          [0.2312, 0.2402, 0.2471,  ..., 0.2515, 0.2471, 0.2350],\n",
      "          ...,\n",
      "          [0.2308, 0.2380, 0.2476,  ..., 0.2475, 0.2442, 0.2384],\n",
      "          [0.2293, 0.2320, 0.2371,  ..., 0.2419, 0.2415, 0.2332],\n",
      "          [0.2266, 0.2235, 0.2261,  ..., 0.2305, 0.2336, 0.2289]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.2325, 0.2396, 0.2451,  ..., 0.2440, 0.2419, 0.2344],\n",
      "          [0.2305, 0.2425, 0.2478,  ..., 0.2516, 0.2482, 0.2402],\n",
      "          [0.2312, 0.2374, 0.2481,  ..., 0.2534, 0.2479, 0.2406],\n",
      "          ...,\n",
      "          [0.2321, 0.2409, 0.2469,  ..., 0.2485, 0.2458, 0.2377],\n",
      "          [0.2290, 0.2320, 0.2367,  ..., 0.2434, 0.2412, 0.2348],\n",
      "          [0.2261, 0.2243, 0.2263,  ..., 0.2329, 0.2297, 0.2317]]]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class UNet_M(Module):\n",
    "    def __init__(self, in_channel=1, out_channel=2, depth=3, n_filters=64, kernel_size=3, attention=True, \n",
    "                r_ratio=16, att_kernel_size=7):\n",
    "        \n",
    "        \"\"\"\n",
    "        in_channel (int): Number of channels in the input image.\n",
    "        out_channel (int): Number of channels in the output image.\n",
    "        depth (int): Number of down convolutions minus bottom down convolution.\n",
    "        n_filters (int): Number of base filters in the U-Net.\n",
    "        kernel_size (int):kernel size for convolution process in the U-Net (paper kernel_size=3)\n",
    "        attention (bool):If attention=True, attention Module is added in the decoder to help focus \n",
    "        attention on important features. Code related to the attentions gates is inspired from:\n",
    "        https://github.com/ozan-oktay/Attention-Gated-Networks\n",
    "        r_ratio (int): reduction ratio that influences the hidden layer neuron number. it reduces the \n",
    "        number of neurons by factor of r_ratio \n",
    "        att_kernel_size (int):  indicated the kernel size fo the convolution filtering in spatial \n",
    "        attention (size in paper=7)\n",
    "\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        super(UNet_M,self).__init__()\n",
    "        self.encoder = ENCODER(in_channel=in_channel, depth=depth, n_filters=n_filters, kernel_size=kernel_size\n",
    "                               ,attention= attention, r_ratio=r_ratio, att_kernel_size=att_kernel_size)\n",
    "        self.decoder = DECODER(out_channel=out_channel, depth=depth, n_filters=n_filters, kernel_size=kernel_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        feat = self.encoder(x)\n",
    "        pred = self.decoder(feat)\n",
    "        \n",
    "        print(pred.size())\n",
    "        return pred\n",
    "        \n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    image = torch.rand((2,1,322,322)) # batch, Channle, W, H as input\n",
    "    model = UNet_M()\n",
    "    print(model(image))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
