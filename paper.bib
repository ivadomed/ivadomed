@inproceedings{wang_adaptive_2019,
  title={Adaptive wing loss for robust face alignment via heatmap regression},
  author={Wang, Xinyao and Bo, Liefeng and Fuxin, Li},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={6971--6981},
  year={2019},
  doi={10.1109/iccv.2019.00707}
}


@article{calabrese_postmortem_2018,
	title = {Postmortem diffusion {MRI} of the entire human spinal cord at microscopic resolution},
	volume = {18},
	issn = {2213-1582},
	url = {http://www.sciencedirect.com/science/article/pii/S2213158218300962},
	doi = {10.1016/j.nicl.2018.03.029},
	abstract = {The human spinal cord is a central nervous system structure that plays an important role in normal motor and sensory function, and can be affected by many debilitating neurologic diseases. Due to its clinical importance, the spinal cord is frequently the subject of imaging research. Common methods for visualizing spinal cord anatomy and pathology include histology and magnetic resonance imaging (MRI), both of which have unique benefits and drawbacks. Postmortem microscopic resolution MRI of fixed specimens, sometimes referred to as magnetic resonance microscopy (MRM), combines many of the benefits inherent to both techniques. However, the elongated shape of the human spinal cord, along with hardware and scan time limitations, have restricted previous microscopic resolution MRI studies (both in vivo and ex vivo) to small sections of the cord. Here we present the first MRM dataset of the entire postmortem human spinal cord. These data include 50 μm isotropic resolution anatomic image data and 100 μm isotropic resolution diffusion data, made possible by a 280 h long multi-segment acquisition and automated image segment composition. We demonstrate the use of these data for spinal cord lesion detection, automated volumetric gray matter segmentation, and quantitative spinal cord morphometry including estimates of cross sectional dimensions and gray matter fraction throughout the length of the cord.},
	language = {en},
	urldate = {2020-10-13},
	journal = {NeuroImage: Clinical},
	author = {Calabrese, Evan and Adil, Syed M. and Cofer, Gary and Perone, Christian S. and Cohen-Adad, Julien and Lad, Shivanand P. and Johnson, G. Allan},
	month = jan,
	year = {2018},
	keywords = {Gray matter, Human, Magnetic resonance microscopy, Spinal cord, Tractography},
	pages = {963--971},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Lucas\\Zotero\\storage\\BEXXPKK2\\Calabrese et al. - 2018 - Postmortem diffusion MRI of the entire human spina.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Lucas\\Zotero\\storage\\U8WIWPXF\\S2213158218300962.html:text/html}
}

@article{gros_automatic_2019,
	title = {Automatic segmentation of the spinal cord and intramedullary multiple sclerosis lesions with convolutional neural networks},
	volume = {184},
	issn = {1053-8119},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6759925/},
	doi = {10.1016/j.neuroimage.2018.09.081},
	abstract = {The spinal cord is frequently affected by atrophy and/or lesions in multiple sclerosis (MS) patients. Segmentation of the spinal cord and lesions from MRI data provides measures of damage, which are key criteria for the diagnosis, prognosis, and longitudinal monitoring in MS. Automating this operation eliminates inter-rater variability and increases the efficiency of large-throughput analysis pipelines. Robust and reliable segmentation across multi-site spinal cord data is challenging because of the large variability related to acquisition parameters and image artifacts. In particular, a precise delineation of lesions is hindered by a broad heterogeneity of lesion contrast, size, location, and shape. The goal of this study was to develop a fully-automatic framework — robust to variability in both image parameters and clinical condition — for segmentation of the spinal cord and intramedullary MS lesions from conventional MRI data of MS and non-MS cases. Scans of 1,042 subjects (459 healthy controls, 471 MS patients, and 112 with other spinal pathologies) were included in this multi-site study (n=30). Data spanned three contrasts (T1-, T2-, and T2*-weighted) for a total of 1,943 volumes and featured large heterogeneity in terms of resolution, orientation, coverage, and clinical conditions. The proposed cord and lesion automatic segmentation approach is based on a sequence of two Convolutional Neural Networks (CNNs). To deal with the very small proportion of spinal cord and/or lesion voxels compared to the rest of the volume, a first CNN with 2D dilated convolutions detects the spinal cord centerline, followed by a second CNN with 3D convolutions that segments the spinal cord and/or lesions. CNNs were trained independently with the Dice loss. When compared against manual segmentation, our CNN-based approach showed a median Dice of 95\% vs. 88\% for PropSeg (p≤0.05), a state-of-the-art spinal cord segmentation method. Regarding lesion segmentation on MS data, our framework provided a Dice of 60\%, a relative volume difference of −15\%, and a lesion-wise detection sensitivity and precision of 83\% and 77\%, respectively. In this study, we introduce a robust method to segment the spinal cord and intramedullary MS lesions on a variety of MRI contrasts. The proposed framework is open-source and readily available in the Spinal Cord Toolbox.},
	urldate = {2020-05-18},
	journal = {NeuroImage},
	author = {Gros, Charley and De Leener, Benjamin and Badji, Atef and Maranzano, Josefina and Eden, Dominique and Dupont, Sara M. and Talbott, Jason and Zhuoquiong, Ren and Liu, Yaou and Granberg, Tobias and Ouellette, Russell and Tachibana, Yasuhiko and Hori, Masaaki and Kamiya, Kouhei and Chougar, Lydia and Stawiarz, Leszek and Hillert, Jan and Bannier, Elise and Kerbrat, Anne and Edan, Gilles and Labauge, Pierre and Callot, Virginie and Pelletier, Jean and Audoin, Bertrand and Rasoanandrianina, Henitsoa and Brisset, Jean-Christophe and Valsasina, Paola and Rocca, Maria A. and Filippi, Massimo and Bakshi, Rohit and Tauhid, Shahamat and Prados, Ferran and Yiannakas, Marios and Kearney, Hugh and Ciccarelli, Olga and Smith, Seth and Treaba, Constantina Andrada and Mainero, Caterina and Lefeuvre, Jennifer and Reich, Daniel S. and Nair, Govind and Auclair, Vincent and McLaren, Donald G. and Martin, Allan R. and Fehlings, Michael G. and Vahdat, Shahabeddin and Khatibi, Ali and Doyon, Julien and Shepherd, Timothy and Charlson, Erik and Narayanan, Sridar and Cohen-Adad, Julien},
	month = jan,
	year = {2019},
	pmid = {30300751},
	pmcid = {PMC6759925},
	pages = {901--915},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Lucas\\Zotero\\storage\\BEKHXIPR\\Gros et al. - 2019 - Automatic segmentation of the spinal cord and intr.pdf:application/pdf}
}

@article{paszke2017automatic,
  title={Automatic differentiation in PyTorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  journal = {NeurIPS Workshop},
  year={2017}
}

@article{kim_deep_2019,
	title = {Deep {Learning} in {Medical} {Imaging}},
	volume = {16},
	issn = {2586-6583},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6945006/},
	doi = {10.14245/ns.1938396.198},
	abstract = {The artificial neural network (ANN), one of the machine learning (ML) algorithms, inspired by the human brain system, was developed by connecting layers with artificial neurons. However, due to the low computing power and insufficient learnable data, ANN has suffered from overfitting and vanishing gradient problems for training deep networks. The advancement of computing power with graphics processing units and the availability of large data acquisition, deep neural network outperforms human or other ML capabilities in computer vision and speech recognition tasks. These potentials are recently applied to healthcare problems, including computer-aided detection/diagnosis, disease prediction, image segmentation, image generation, etc. In this review article, we will explain the history, development, and applications in medical imaging},
	number = {4},
	urldate = {2020-09-17},
	journal = {Neurospine},
	author = {Kim, Mingyu and Yun, Jihye and Cho, Yongwon and Shin, Keewon and Jang, Ryoungwoo and Bae, Hyun-jin and Kim, Namkug},
	month = dec,
	year = {2019},
	pmid = {31905454},
	pmcid = {PMC6945006},
	pages = {657--668},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Lucas\\Zotero\\storage\\N8262477\\Kim et al. - 2019 - Deep Learning in Medical Imaging.pdf:application/pdf}
}


@article{bids_2016,
	title = {The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments},
	volume = {3},
	copyright = {2016 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/sdata201644},
	doi = {10.1038/sdata.2016.44},
	abstract = {The development of magnetic resonance imaging (MRI) techniques has defined modern neuroimaging. Since its inception, tens of thousands of studies using techniques such as functional MRI and diffusion weighted imaging have allowed for the non-invasive study of the brain. Despite the fact that MRI is routinely used to obtain data for neuroscience research, there has been no widely adopted standard for organizing and describing the data collected in an imaging experiment. This renders sharing and reusing data (within or between labs) difficult if not impossible and unnecessarily complicates the application of automatic pipelines and quality assurance protocols. To solve this problem, we have developed the Brain Imaging Data Structure (BIDS), a standard for organizing and describing MRI datasets. The BIDS standard uses file formats compatible with existing software, unifies the majority of practices already common in the field, and captures the metadata necessary for most common data processing operations.},
	language = {en},
	number = {1},
	urldate = {2020-09-21},
	journal = {Scientific Data},
	author = {Gorgolewski, Krzysztof J. and Auer, Tibor and Calhoun, Vince D. and Craddock, R. Cameron and Das, Samir and Duff, Eugene P. and Flandin, Guillaume and Ghosh, Satrajit S. and Glatard, Tristan and Halchenko, Yaroslav O. and Handwerker, Daniel A. and Hanke, Michael and Keator, David and Li, Xiangrui and Michael, Zachary and Maumet, Camille and Nichols, B. Nolan and Nichols, Thomas E. and Pellman, John and Poline, Jean-Baptiste and Rokem, Ariel and Schaefer, Gunnar and Sochat, Vanessa and Triplett, William and Turner, Jessica A. and Varoquaux, Gaël and Poldrack, Russell A.},
	month = jun,
	year = {2016},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {160044},
	file = {Full Text PDF:C\:\\Users\\Lucas\\Zotero\\storage\\TNSCXQJX\\Gorgolewski et al. - 2016 - The brain imaging data structure, a format for org.pdf:application/pdf}
}



@inproceedings{lemay_fully_2020,
	address = {OHBM Annual meeting},
	title = {Fully {Automatic} {Spinal} {Cord} {Tumor} {Segmentation} on {MRI} with {Deep} {Learning}},
	author = {Lemay, Andreanne and Gros, Charley and Zhuo, Zhizheng and Zhang, Jie and Duan, Yunyun and Paperi, Sara and Cohen-Adad, Julien and Liu, Yaou},
    booktitle={Organization for Human Brain Mapping},
	month = jul,
	year = {2020}
}


@article{,
  title={Exploring uncertainty measures in deep networks for multiple sclerosis lesion detection and segmentation},
  author={Nair, Tanya and Precup, Doina and Arnold, Douglas L and Arbel, Tal},
  journal={Medical image analysis},
  volume={59},
  pages={101557},
  year={2020},
  publisher={Elsevier},
  doi={10.1016/j.media.2019.101557}
}


@article{wang_aleatoric_2019,
	title = {Aleatoric uncertainty estimation with test-time augmentation for medical image segmentation with convolutional neural networks},
	volume = {338},
	issn = {0925-2312},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231219301961},
	doi = {10.1016/j.neucom.2019.01.103},
	abstract = {Despite the state-of-the-art performance for medical image segmentation, deep convolutional neural networks (CNNs) have rarely provided uncertainty estimations regarding their segmentation outputs, e.g., model (epistemic) and image-based (aleatoric) uncertainties. In this work, we analyze these different types of uncertainties for CNN-based 2D and 3D medical image segmentation tasks at both pixel level and structure level. We additionally propose a test-time augmentation-based aleatoric uncertainty to analyze the effect of different transformations of the input image on the segmentation output. Test-time augmentation has been previously used to improve segmentation accuracy, yet not been formulated in a consistent mathematical framework. Hence, we also propose a theoretical formulation of test-time augmentation, where a distribution of the prediction is estimated by Monte Carlo simulation with prior distributions of parameters in an image acquisition model that involves image transformations and noise. We compare and combine our proposed aleatoric uncertainty with model uncertainty. Experiments with segmentation of fetal brains and brain tumors from 2D and 3D Magnetic Resonance Images (MRI) showed that 1) the test-time augmentation-based aleatoric uncertainty provides a better uncertainty estimation than calculating the test-time dropout-based model uncertainty alone and helps to reduce overconfident incorrect predictions, and 2) our test-time augmentation outperforms a single-prediction baseline and dropout-based multiple predictions.},
	language = {en},
	urldate = {2020-09-24},
	journal = {Neurocomputing},
	author = {Wang, Guotai and Li, Wenqi and Aertsen, Michael and Deprest, Jan and Ourselin, Sébastien and Vercauteren, Tom},
	month = apr,
	year = {2019},
	keywords = {Convolutional neural networks, Data augmentation, Medical image segmentation, Uncertainty estimation},
	pages = {34--45},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Lucas\\Zotero\\storage\\AAWRLVKP\\Wang et al. - 2019 - Aleatoric uncertainty estimation with test-time au.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Lucas\\Zotero\\storage\\GCZC4GUF\\S0925231219301961.html:text/html}
}

@article{oktay2018attention,
  title={Attention u-net: Learning where to look for the pancreas},
  author={Oktay, Ozan and Schlemper, Jo and Folgoc, Loic Le and Lee, Matthew and Heinrich, Mattias and Misawa, Kazunari and Mori, Kensaku and McDonagh, Steven and Hammerla, Nils Y and Kainz, Bernhard and others},
  journal={arXiv preprint arXiv:1804.03999},
  year={2018}
}

@article{zhang2017mixup,
  title={mixup: Beyond empirical risk minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1710.09412},
  year={2017}
}

@article{perez2017film,
  title={Film: Visual reasoning with a general conditioning layer},
  author={Perez, Ethan and Strub, Florian and De Vries, Harm and Dumoulin, Vincent and Courville, Aaron},
  journal={arXiv preprint arXiv:1709.07871},
  year={2017}
}


@article{havaei2016hemis,
  title={HeMIS: Hetero-Modal Image Segmentation},
  author={Havaei, Mohammad and Guizard, Nicolas and Chapados, Nicolas and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1607.05194},
  year={2016},
  doi={10.1007/978-3-319-46723-8_54}
}


@inproceedings{Cohen2017countception,
abstract = {Counting objects in digital images is a process that should be replaced by machines. This tedious task is time consuming and prone to errors due to fatigue of human annotators. The goal is to have a system that takes as input an image and returns a count of the objects inside and justification for the prediction in the form of object localization. We repose a problem, originally posed by Lempitsky and Zisserman, to instead predict a count map which contains redundant counts based on the receptive field of a smaller regression network. The regression network predicts a count of the objects that exist inside this frame. By processing the image in a fully convolutional way each pixel is going to be accounted for some number of times, the number of windows which include it, which is the size of each window, (i.e., 32x32 = 1024). To recover the true count we take the average over the redundant predictions. Our contribution is redundant counting instead of predicting a density map in order to average over errors. We also propose a novel deep neural network architecture adapted from the Inception family of networks called the Count-ception network. Together our approach results in a 20{\%} relative improvement (2.9 to 2.3 MAE) over the state of the art method by Xie, Noble, and Zisserman in 2016.},
archivePrefix = {arXiv},
arxivId = {1703.08710},
author = {Cohen, Joseph Paul and Boucher, Genevieve and Glastonbury, Craig A. and Lo, Henry Z. and Bengio, Yoshua},
booktitle = {International Conference on Computer Vision Workshop on BioImage Computing},
eprint = {1703.08710},
file = {:Users/ieee8023/Library/Application Support/Mendeley Desktop/Downloaded/Cohen et al. - 2017 - Count-ception Counting by Fully Convolutional Redundant Counting.pdf:pdf},
mendeley-groups = {neuralnetworks/counting/core-counting},
title = {{Count-ception: Counting by Fully Convolutional Redundant Counting}},
url = {http://arxiv.org/abs/1703.08710},
year = {2017},
doi = {10.1109/iccvw.2017.9}
}


@inproceedings{Huang2017densenet,
archivePrefix = {arXiv},
arxivId = {1608.06993},
author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
booktitle = {Computer Vision and Pattern Recognition},
eprint = {1608.06993},
file = {:Users/ieee8023/Library/Application Support/Mendeley Desktop/Downloaded/Huang et al. - 2016 - Densely Connected Convolutional Networks.pdf:pdf},
mendeley-groups = {neuralnetworks/vision},
title = {{Densely Connected Convolutional Networks}},
url = {https://arxiv.org/abs/1608.06993},
year = {2017}
}

@inproceedings{Ronneberger2015unet,
archivePrefix = {arXiv},
arxivId = {1505.04597},
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
booktitle = {Medical Image Computing and Computer Assisted Intervention},
eprint = {1505.04597},
file = {:Users/ieee8023/Library/Application Support/Mendeley Desktop/Downloaded/Ronneberger, Fischer, Brox - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf:pdf},
mendeley-groups = {neuralnetworks/seg},
month = {may},
title = {{U-Net: Convolutional Networks for Biomedical Image Segmentation}},
url = {https://arxiv.org/abs/1505.04597},
year = {2015}
}

@inproceedings{he2016resnet,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learn- ing residual functions with reference to the layer inputs, in- stead of learning unreferenced functions. We provide com- prehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complex- ity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our ex- tremely deep representations, we obtain a 28{\%} relative im- provement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet local- ization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.90},
eprint = {1512.03385},
isbn = {978-1-4673-8851-1},
issn = {1664-1078},
mendeley-groups = {neuralnetworks/ranking,blind},
pages = {770--778},
pmid = {23554596},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://ieeexplore.ieee.org/document/7780459/},
year = {2016}
}

@article{roy_quicknat_2018,
	title = {{QuickNAT}: {A} {Fully} {Convolutional} {Network} for {Quick} and {Accurate} {Segmentation} of {Neuroanatomy}},
	shorttitle = {{QuickNAT}},
	url = {http://arxiv.org/abs/1801.04161},
	abstract = {Whole brain segmentation from structural magnetic resonance imaging (MRI) is a prerequisite for most morphological analyses, but is computationally intense and can therefore delay the availability of image markers after scan acquisition. We introduce QuickNAT, a fully convolutional, densely connected neural network that segments a {\textbackslash}revision\{MRI brain scan\} in 20 seconds. To enable training of the complex network with millions of learnable parameters using limited annotated data, we propose to first pre-train on auxiliary labels created from existing segmentation software. Subsequently, the pre-trained model is fine-tuned on manual labels to rectify errors in auxiliary labels. With this learning strategy, we are able to use large neuroimaging repositories without manual annotations for training. In an extensive set of evaluations on eight datasets that cover a wide age range, pathology, and different scanners, we demonstrate that QuickNAT achieves superior segmentation accuracy and reliability in comparison to state-of-the-art methods, while being orders of magnitude faster. The speed up facilitates processing of large data repositories and supports translation of imaging biomarkers by making them available within seconds for fast clinical decision making.},
	urldate = {2020-09-28},
	journal = {arXiv:1801.04161 [cs]},
	doi = {10.1016/j.neuroimage.2018.11.042},
	author = {Roy, Abhijit Guha and Conjeti, Sailesh and Navab, Nassir and Wachinger, Christian},
	month = nov,
	year = {2018},
	note = {arXiv: 1801.04161},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted for Publication at NeuroImage},
	file = {arXiv Fulltext PDF:C\:\\Users\\Lucas\\Zotero\\storage\\864K65TV\\Roy et al. - 2018 - QuickNAT A Fully Convolutional Network for Quick .pdf:application/pdf}
}


@inproceedings{isensee2017brain,
  title={Brain tumor segmentation and radiomics survival prediction: Contribution to the \uppercase{BRATS} 2017 challenge},
  author={Isensee, Fabian and Kickingereder, Philipp and Wick, Wolfgang and Bendszus, Martin and Maier-Hein, Klaus H},
  booktitle={International MICCAI Brainlesion Workshop},
  pages={287--297},
  year={2017},
  organization={Springer}
}


@inproceedings{milletari2016v,
  title={V-net: Fully convolutional neural networks for volumetric medical image segmentation},
  author={Milletari, Fausto and Navab, Nassir and Ahmadi, Seyed-Ahmad},
  booktitle={2016 fourth international conference on 3D vision (3DV)},
  pages={565--571},
  doi={10.1109/3dv.2016.79},
  year={2016},
  organization={IEEE}
}


@inproceedings{wong20183d,
  title={3D segmentation with exponential logarithmic loss for highly unbalanced object sizes},
  author={Wong, Ken CL and Moradi, Mehdi and Tang, Hui and Syeda-Mahmood, Tanveer},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={612--619},
  year={2018},
  organization={Springer},
  doi={10.1007/978-3-030-00931-1_70}
}

@ARTICLE{Willemink2020-au,
  title    = "Preparing Medical Imaging Data for Machine Learning",
  author   = "Willemink, Martin J and Koszek, Wojciech A and Hardell, Cailin
              and Wu, Jie and Fleischmann, Dominik and Harvey, Hugh and Folio,
              Les R and Summers, Ronald M and Rubin, Daniel L and Lungren,
              Matthew P",
  abstract = "Artificial intelligence (AI) continues to garner substantial
              interest in medical imaging. The potential applications are vast
              and include the entirety of the medical imaging life cycle from
              image creation to diagnosis to outcome prediction. The chief
              obstacles to development and clinical implementation of AI
              algorithms include availability of sufficiently large, curated,
              and representative training data that includes expert labeling
              (eg, annotations). Current supervised AI methods require a
              curation process for data to optimally train, validate, and test
              algorithms. Currently, most research groups and industry have
              limited data access based on small sample sizes from small
              geographic areas. In addition, the preparation of data is a
              costly and time-intensive process, the results of which are
              algorithms with limited utility and poor generalization. In this
              article, the authors describe fundamental steps for preparing
              medical imaging data in AI algorithm development, explain current
              limitations to data curation, and explore new approaches to
              address the problem of data availability.",
  journal  = "Radiology",
  volume   =  295,
  number   =  1,
  pages    = "4--15",
  month    =  apr,
  year     =  2020,
  language = "en"
}
